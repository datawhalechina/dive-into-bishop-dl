
本章中我们考虑**分类问题**。所谓分类问题是指给一个 $D$ 维的特征向量 $\boldsymbol{x} \in \mathbb{R}^{D}$，我们需要得出它属于 $K$ 个类别 $\mathcal{C}_{K}$ 中的哪一个。一般我们假设一个特征向量不可能同时属于两类，因此整个输入空间 $\mathbb{R}^{D}$ 就被划分为了若干部分。这些部分称为 **决策区域**，分割这些区域的边界称为 **决策边界**。我们在本章中考虑**线性模型**，它们拥有线性的决策边界，也就是 $D-1$ 维的 **超平面**。我们称可以被线性分类模型完全分开的数据集是 **线性可分的**。

我们可以想到多方式来完成分类这个任务：第一种最简单，给定输入，直接输出它所在的分类。第二种稍微复杂一些，它在推断阶段建模一个条件分布 $p(\mathcal{C}_{K}|\boldsymbol{x})$，然后在决策阶段得到分类结果。得到这个条件分布的方式也有两种。第一种是定义一个带可学习参数的模型，然后用机器学习的方法直接建模 $p(\mathcal{C}_{K}, \boldsymbol{x})$ 这称为 **判别式分类器**；另一种使用 Bayes 公式，分别建模 $p(\mathcal{C}_{K})$ 和 $p(\boldsymbol{x}|\mathcal{C}_{K})$，最后得到 $p(\mathcal{C}_{K}|\boldsymbol{x})$，这被称为 **生成式分类器**，因为我们有了 $p(\boldsymbol{x}|\mathcal{C}_{K})$，就能给定一个类，然后属于该类的特征向量了。

## 1 二分类

我们首先考虑最简单的情况——二分类。在这个条件下，给定输入向量 $\boldsymbol{x}$，判别器的输出只可能是 $\mathcal{C}_{1}$ 或是 $\mathcal{C}_{2}$。最简单的判别器是线性判别器。它长得和线性回归的模型很像
$$
y(\boldsymbol{x}) = \boldsymbol{w}^{\top}\boldsymbol{x} + w_{0}
$$
这并不直接得到分类的结果，不妨这样决定
$$
\text{class}(\boldsymbol{x}) = \begin{cases}
\mathcal{C}_{1} & y(\boldsymbol{x}) \geqslant 0\\
\mathcal{C}_{2} & y(\boldsymbol{x}) < 0.
\end{cases}
$$
此时我们可以看到，决定输入是属于 $\mathcal{C}_{1}$ 还是 $\mathcal{C}_{2}$ 的是特征点是否在某个超平面 $\boldsymbol{w}^{\top}\boldsymbol{x} + w_{0} = 0$ 的一边。这个超平面本身就是 **决策边界**。接下来我们考虑在决策边界上的任意两点 $\boldsymbol{x}_{A}$ 和 $\boldsymbol{x}_{B}$，显然我们有 $\boldsymbol{w}^{\top}\boldsymbol{x}_{A} + w_{0} = \boldsymbol{w}^{\top}\boldsymbol{x}_{B} + w_{0} = 0$，进而有 $\boldsymbol{w}^{\top}(\boldsymbol{x}_{A} - \boldsymbol{x}_{B}) = 0$，这说明 $\boldsymbol{w}$ 和 $\boldsymbol{x}_{A} - \boldsymbol{x}_{B}$ 正交，并决定了超平面的 “方向“。另外，由于 $\boldsymbol{w}^{\top}\boldsymbol{x}_{B} + w_{0} = 0$，假设 $\boldsymbol{w}$ 不为零，就有 $\displaystyle \frac{\boldsymbol{w}^{\top}\boldsymbol{x}_{B}}{\| \boldsymbol{w} \|_{2}} = -\frac{w_{0}}{\| \boldsymbol{w} \|_{2}}$，这被称为超平面到原点的 **距离**，如下图所示；因此当偏置项 $w_{0}$ 不为零时，超平面并不经过原点。

<center>
<img src="./ch5/attachments/Pasted%20image%2020250615223813.png" style="zoom:50%;" alt=""/>
</center>

类似地，我们也可以通过 $y(\boldsymbol{x})$ 得到输入向量（终点）离超平面的距离。我们首先将 $\boldsymbol{x}$ 分解为与超平面垂直的 $\boldsymbol{x}_{\parallel}$ （它同时与 $\boldsymbol{w}$ 平行）以及与超平面平行（与 $\boldsymbol{w}$ 垂直）的 $\boldsymbol{x}_{\perp}$，由于 $\boldsymbol{x}_{\perp}$ 和 $\boldsymbol{w}$ 平行，一定存在某个实数 $r$，使得 $\displaystyle \boldsymbol{x}_{\parallel} = r \frac{\boldsymbol{w}}{\| \boldsymbol{w} \|_{2}}$，因此
$$
y(\boldsymbol{x}) = \boldsymbol{w}^{\top}(\boldsymbol{x}_{\perp} + \boldsymbol{x}_{\parallel}) + w_{0} = \boldsymbol{w}^{\top}\boldsymbol{x}_{\parallel} \iff \frac{y(\boldsymbol{x})}{\| \boldsymbol{w} \|_{2} } = r\boldsymbol{w}^{\top} \frac{\boldsymbol{w}}{\| \boldsymbol{w} \|_{2} } = r
$$
这个 $r$ 就是 $\boldsymbol{x}$ 到超平面之间的距离。

为了方便起见，我们定义 $\tilde{\boldsymbol{x}} = [x_{0}, \boldsymbol{x}]^{\top}$，$\tilde{\boldsymbol{w}} = [w_{0}, \boldsymbol{w}]^{\top}$，这样原来的函数 $y$ 就可以写成
$$
y(\boldsymbol{x}) = \tilde{\boldsymbol{w}}^{\top}\tilde{\boldsymbol{x}}.
$$

## 2 多分类


## 3 $1$-of-$K$ 编码方案


## 4 最小二乘分类

