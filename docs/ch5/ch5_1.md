
本章中我们考虑**分类问题**。所谓分类问题是指给一个 $D$ 维的特征向量 $\boldsymbol{x} \in \mathbb{R}^{D}$，我们需要得出它属于 $K$ 个类别 $\mathcal{C}_{K}$ 中的哪一个。一般我们假设一个特征向量不可能同时属于两类，因此整个输入空间 $\mathbb{R}^{D}$ 就被划分为了若干部分。这些部分称为 **决策区域**，分割这些区域的边界称为 **决策边界**。我们在本章中考虑**线性模型**，它们拥有线性的决策边界，也就是 $D-1$ 维的 **超平面**。我们称可以被线性分类模型完全分开的数据集是 **线性可分的**。

我们可以想到多方式来完成分类这个任务：第一种最简单，给定输入，直接输出它所在的分类。第二种稍微复杂一些，它在推断阶段建模一个条件分布 $p(\mathcal{C}_{K}|\boldsymbol{x})$，然后在决策阶段得到分类结果。得到这个条件分布的方式也有两种。第一种是定义一个带可学习参数的模型，然后用机器学习的方法直接建模 $p(\mathcal{C}_{K}, \boldsymbol{x})$ 这称为 **判别式分类器**；另一种使用 Bayes 公式，分别建模 $p(\mathcal{C}_{K})$ 和 $p(\boldsymbol{x}|\mathcal{C}_{K})$，最后得到 $p(\mathcal{C}_{K}|\boldsymbol{x})$，这被称为 **生成式分类器**，因为我们有了 $p(\boldsymbol{x}|\mathcal{C}_{K})$，就能给定一个类，然后属于该类的特征向量了。

## 1 二分类

我们首先考虑最简单的情况——二分类。在这个条件下，给定输入向量 $\boldsymbol{x}$，判别器的输出只可能是 $\mathcal{C}_{1}$ 或是 $\mathcal{C}_{2}$。最简单的判别器是线性判别器。它长得和线性回归的模型很像
$$
y(\boldsymbol{x}) = \boldsymbol{w}^{\top}\boldsymbol{x} + w_{0}
$$
这并不直接得到分类的结果，不妨这样决定
$$
\text{class}(\boldsymbol{x}) = \begin{cases}
\mathcal{C}_{1} & y(\boldsymbol{x}) \geqslant 0\\
\mathcal{C}_{2} & y(\boldsymbol{x}) < 0.
\end{cases}
$$
此时我们可以看到，决定输入是属于 $\mathcal{C}_{1}$ 还是 $\mathcal{C}_{2}$ 的是特征点是否在某个超平面 $\boldsymbol{w}^{\top}\boldsymbol{x} + w_{0} = 0$ 的一边。这个超平面本身就是 **决策边界**。接下来我们考虑在决策边界上的任意两点 $\boldsymbol{x}_{A}$ 和 $\boldsymbol{x}_{B}$，显然我们有 $\boldsymbol{w}^{\top}\boldsymbol{x}_{A} + w_{0} = \boldsymbol{w}^{\top}\boldsymbol{x}_{B} + w_{0} = 0$，进而有 $\boldsymbol{w}^{\top}(\boldsymbol{x}_{A} - \boldsymbol{x}_{B}) = 0$，这说明 $\boldsymbol{w}$ 和 $\boldsymbol{x}_{A} - \boldsymbol{x}_{B}$ 正交，并决定了超平面的 “方向“。另外，由于 $\boldsymbol{w}^{\top}\boldsymbol{x}_{B} + w_{0} = 0$，假设 $\boldsymbol{w}$ 不为零，就有 $\displaystyle \frac{\boldsymbol{w}^{\top}\boldsymbol{x}_{B}}{\| \boldsymbol{w} \|_{2}} = -\frac{w_{0}}{\| \boldsymbol{w} \|_{2}}$，这被称为超平面到原点的 **距离**，如下图所示；因此当偏置项 $w_{0}$ 不为零时，超平面并不经过原点。

<center>
<img src="./ch5/attachments/Pasted%20image%2020250615223813.png" style="zoom:50%;" alt=""/>
</center>

类似地，我们也可以通过 $y(\boldsymbol{x})$ 得到输入向量（终点）离超平面的距离。我们首先将 $\boldsymbol{x}$ 分解为与超平面垂直的 $\boldsymbol{x}_{\parallel}$ （它同时与 $\boldsymbol{w}$ 平行）以及与超平面平行（与 $\boldsymbol{w}$ 垂直）的 $\boldsymbol{x}_{\perp}$，由于 $\boldsymbol{x}_{\perp}$ 和 $\boldsymbol{w}$ 平行，一定存在某个实数 $r$，使得 $\displaystyle \boldsymbol{x}_{\parallel} = r \frac{\boldsymbol{w}}{\| \boldsymbol{w} \|_{2}}$，因此
$$
y(\boldsymbol{x}) = \boldsymbol{w}^{\top}(\boldsymbol{x}_{\perp} + \boldsymbol{x}_{\parallel}) + w_{0} = \boldsymbol{w}^{\top}\boldsymbol{x}_{\parallel} \iff \frac{y(\boldsymbol{x})}{\| \boldsymbol{w} \|_{2} } = r\boldsymbol{w}^{\top} \frac{\boldsymbol{w}}{\| \boldsymbol{w} \|_{2} } = r
$$
这个 $r$ 就是 $\boldsymbol{x}$ 到超平面之间的距离。

为了方便起见，我们定义 $\tilde{\boldsymbol{x}} = [x_{0}, \boldsymbol{x}]^{\top}$，$\tilde{\boldsymbol{w}} = [w_{0}, \boldsymbol{w}]^{\top}$，这样原来的函数 $y$ 就可以写成
$$
y(\boldsymbol{x}) = \tilde{\boldsymbol{w}}^{\top}\tilde{\boldsymbol{x}}.
$$

## 2 多分类

现在我们考虑多分类，也就是 $K > 2$ 时的情形。我们可能会想让多个二分类器组合起来来承担多分类的任务，但这事实上会出问题 (Duda and Hart, 1973)。

我们可以这样考虑，构建 $K - 1$ 各分类器，每个分类器回答“输入是否属于我这一个类” (one-v.s.-the-rest ) 的问题，如下图所示。相比之下，如果我们对每一对类都构造一个分类器 (one-v.s.-one)，那我们需要构建 $\displaystyle\frac{K(K-1)}{2}$ 个分类器。这在类别数变得很多（如 ImageNet 的 $1000$ 类）时候开销会大的吓人，这显然不现实。但这并不说明前者就更好了。如下图所示，两种方法都会在输入空间中出现一个“无人认领”的区域：我们不知道这块区域中的输入属于哪一个类：直观来看，三分类时我们应该将输入空间分成三个部分，而我们用二分类器组合永远也不可能把输入空间分成三块。

<center>
<img src="./ch5/attachments/Pasted%20image%2020250618132537.png" style="zoom:50%;" alt=""/>
</center>
<center>one-v.s.-the-rest 和 one-vs-one 分类器的对比</center>

怎么办呢，我们想一个中学班级有若干学生，然后我们根据他们的每科成绩高低来判断每个学生擅长哪个学科（我们假设每个学科的总分一样，班级得分均值和标准差也一样，且每个学生每门课的分数两两不同）。在这样的设定下，每个学生一定有一个分数最高的科目，我们称ta擅长这个科目。假如科目有 $K$ 个，就把班级中的全部学生分成了 $K$ 类，每个学生的成绩可以写成一个向量 $[s_{1}, s_{2}, s_{3}, \dots, s_{K}]$，其擅长的科目就是 $\mathop{\text{arg}}\max\limits_{i}~\{ s_{j} \}_{j=1}^{K}$。这其实也是实际中我们做分类问题的做法，我们定义 $K$ 个函数
$$
y_{k}(\boldsymbol{x}) = \boldsymbol{w}_{k}^{\top}\boldsymbol{x} + w_{k,0}, \quad k = 1, \dots, K
$$
这样给定一个输入向量 $\boldsymbol{x}$，我们能得到 $K$ 个函数值 $y_{1}(\boldsymbol{x}), \dots, y_{K}(\boldsymbol{x})$，它们组成一个向量，我们只需要取最大元在向量中的位置，就能得到分类结果。换句话说，假如对所有的 $j\neq k$，都有 $y_{k}(\boldsymbol{x}) > y_{j}(\boldsymbol{x})$，则这个模型（按照模型的输出）属于类别 $\mathcal{C}_{K}$。此时我们考虑类别的边界。类别 $\mathcal{C}_{j}$ 和 $\mathcal{C}_{k}$ 的边界就是输入使得 $y_{j} = y_{k}$ 的集合：
$$
\boldsymbol{w}_{j}^{\top}\boldsymbol{x} + w_{j,0} = \boldsymbol{w}_{k}^{\top}\boldsymbol{x} + w_{k,0} \iff (\boldsymbol{w}_{j} - \boldsymbol{w}_{k})^{\top}\boldsymbol{x} + (w_{j,0} - w_{k,0}) = 0
$$
是不是很眼熟？假如 $w_{j,0}$ 和 $w_{k,0}$ 都为零，这就说明 $\boldsymbol{x}$ 与 $\boldsymbol{w}_{j} - \boldsymbol{w}_{k}$ 垂直。直观地看，向量 $\boldsymbol{w}_{k}$ 指向了决策区域 $\mathcal{R}_{k}$。决策区域的另一个性质是，它们总是**单连通且凸**的。这是说决策区域 $\mathcal{R}$ 内任意两点 $\boldsymbol{x}_{1}, \boldsymbol{x}_{2}$ 决定的线段都在这个区域内：
$$
\tilde{\boldsymbol{x}} = \lambda \boldsymbol{x}_{1} + (1-\lambda)\boldsymbol{x}_{2} \in \mathcal{R}, \quad  \forall \lambda \in [0, 1]
$$
很粗糙的说，单连通是指这个集合中的任意简单闭曲线（首尾相接，但其他地方不交叉的曲线）的内部都在这个集合中，读者不难发现凸性比单连通要强很多，因此凸性蕴含了决策区域是单连通的。我们可以轻易地证明这件事。假设 $\boldsymbol{x}_{1}, \boldsymbol{x}_{2} \in \mathcal{R}_{k}$ 考虑这两点的判别函数
$$
\begin{align*}
y_{k}(\tilde{\boldsymbol{x}}) &= \boldsymbol{w}_{k}^{\top}(\lambda \boldsymbol{x}_{1} + (1-\lambda)\boldsymbol{x}_{2}) + w_{k,0}\\
&= \lambda(\boldsymbol{w}_{k}^{\top}\boldsymbol{x}_{1} + w_{k,0}) + (1-\lambda)(\boldsymbol{w}_{k}^{\top}\boldsymbol{x}_{2} - w_{k,0})\\
&= \lambda y_{k}(\boldsymbol{x}_{1}) + (1-\lambda)y_{k}(\boldsymbol{x}_{2})
\end{align*}
$$
根据我们的假设，有对任意 $j \neq k$，$y_{k}(\boldsymbol{x}_{1}) > y_{j}(\boldsymbol{x}_{1})$，$y_{k}(\boldsymbol{x}_{2}) > y_{j}(\boldsymbol{x}_{2})$。把前面的不等式乘以 $\lambda$，后面的乘以 $1-\lambda$，就得到 $y_{k}(\tilde{\boldsymbol{x}}) > y_{j}(\tilde{\boldsymbol{x}})$。

<center>
<img src="./ch5/attachments/Pasted%20image%2020250618133250.png" style="zoom:50%;" alt=""/>
</center>
<center>多分类示意图</center>

## 3 $1$-of-$K$ 编码方案

本节我们快速的引入 $1$-of-$K$ 编码方案，一个也许更加耳熟能详的名字是 **one-hot 编码**。在多分类中，我们将类别标签转换为一个向量 $[0, \dots, 0, 1, 0, \dots, 0]^{\top}$。这是有迹可循的。回忆上一节中的多分类的函数 $y_{k}$，假如我们一共有 $K$ 个分类（相应地，分类函数 $y_{k}$ 也有相同的数量），我们也可以像上一章那样将其改写为矩阵形式：
$$
\boldsymbol{y} = \begin{bmatrix}
y_{1}(\boldsymbol{x})\\
y_{2}(\boldsymbol{x})\\
\vdots \\
y_{K}(\boldsymbol{x})
\end{bmatrix} = \begin{bmatrix}
\boldsymbol{w}_{1}^{\top}\boldsymbol{x} + w_{1,0}\\
\boldsymbol{w}_{2}^{\top}\boldsymbol{x} + w_{2,0}\\
\vdots \\
\boldsymbol{w}_{K}^{\top}\boldsymbol{x} + w_{K,0}
\end{bmatrix} = \underbrace{ \begin{bmatrix}
w_{1,0} & -\!\!\!-\boldsymbol{w}_{1}^{\top}-\!\!\!-\\
w_{2,0} & -\!\!\!-\boldsymbol{w}_{2}^{\top}-\!\!\!-\\
\vdots & \vdots \\
w_{K,0}& -\!\!\!-\boldsymbol{w}_{K}^{\top}-\!\!\!-
\end{bmatrix}  }_{ \tilde{\boldsymbol{W}}^{\top} } \,\,
\underbrace{ \begin{bmatrix}
1\\|\\\boldsymbol{x}\\|
\end{bmatrix} }_{ \tilde{\boldsymbol{x}} } = \tilde{\boldsymbol{W}}^{\top}\tilde{\boldsymbol{x}}
$$
我们决定分类结果时，是看输出向量 $\boldsymbol{y}$ 最高的那个分量。我们可以将其视作某种“概率”（假设输出是被归一化的），那对应的标签显然是 “该数据属于该类的概率为 $100\%$ 而属于其他类的概率为零”，因此自然而然的就有上面那样的形式。

## 4 最小二乘分类

在上一章中我们用最小二乘法解回归问题时候得到了一个解析解，这在计算上给我们很大的方便，因为可以一步到位。本节中我们尝试使用类似的方法解多分类问题。在上一节中，我们已经见到了向量版本的分类函数，其中 $\tilde{\boldsymbol{W}} \in \mathbb{R}^{(D+1) \times K}$，$\boldsymbol{x} \in \mathbb{R}^{D+1}$。和上一章一样，我们将输入和目标重构为下面的矩阵
$$
\tilde{\boldsymbol{X}} = \begin{bmatrix}
\tilde{\boldsymbol{x}}_{1}^{\top}\\
\tilde{\boldsymbol{x}}_{2}^{\top}\\
\vdots \\
\tilde{\boldsymbol{x}}_{N}^{\top}
\end{bmatrix} = \begin{bmatrix}
1 & -\!\!\!-\boldsymbol{x}^{\top}_{1}-\!\!\!-\\
1 & -\!\!\!-\boldsymbol{x}^{\top}_{2}-\!\!\!-\\
\vdots  & \vdots \\
1 & -\!\!\!-\boldsymbol{x}^{\top}_{N}-\!\!\!-
\end{bmatrix}, \quad \boldsymbol{T} = \begin{bmatrix}
\boldsymbol{t}_{1}^{\top}\\
\boldsymbol{t}_{2}^{\top}\\
\vdots \\
\boldsymbol{t}_{N}^{\top}
\end{bmatrix}, \quad \boldsymbol{Y} = \tilde{\boldsymbol{X}}\tilde{\boldsymbol{W}}
$$
我们需要对每一个输入 $\tilde{\boldsymbol{x}}$，输出 $\boldsymbol{y}$ 与 目标向量 $\boldsymbol{t}$ 在 Euclid 范数意义下越来越接近，也就是
$$
l = \min~ \frac{1}{2} \| \boldsymbol{y} - \boldsymbol{t} \|_{2}^{2} = \frac{1}{2}\sum\limits_{k=1}^{K} (y_{k} - t_{k})^{2}. 
$$
考虑所有训练样本的总和，我们得到
$$
E_{D}(\tilde{\boldsymbol{W}}) = \frac{1}{2}\sum\limits_{n=1}^{N} \|\boldsymbol{y}_{n} - \boldsymbol{t}_{n}\|_{2}^{2} = \frac{1}{2}\sum\limits_{n=1}^{N} \sum\limits_{k=1}^{K} (y_{n,k} - t_{n,k})^{2}. 
$$
这其实就是 $\boldsymbol{Y}$ 和 $\boldsymbol{T}$ 之差的所有分量的平方和。这其实是矩阵 $\boldsymbol{Y} - \boldsymbol{T}$ 的 Frobenious 范数的平方，后者有下面这样更加优美的形式
$$
E_{D}(\tilde{\boldsymbol{W}}) = \frac{1}{2}\| \boldsymbol{Y} - \boldsymbol{T} \|_{F}^{2} = \frac{1}{2}\text{Tr}\Big[ (\boldsymbol{Y} - \boldsymbol{T})^{\top}(\boldsymbol{Y} - \boldsymbol{T}) \Big] 
$$
其中 $\|\cdot\|_{F}$ 表示 Frobenious 范数，$\text{Tr}(\cdot)$ 表示矩阵的迹（对角元之和），第二个等号读者可以自行证明（非常简单）。接下来我们对 $E_{D}(\tilde{\boldsymbol{W}})$ 求关于 $\tilde{\boldsymbol{W}}$ 的偏导，有
$$
\begin{align}
\displaystyle \frac{ \partial E_{D}(\tilde{\boldsymbol{W}}) }{ \partial \tilde{\boldsymbol{W}} } &= 
\frac{1}{2} \underbrace{ \displaystyle \frac{ \partial\, \text{Tr}\Big[ (\boldsymbol{Y} - \boldsymbol{T})^{\top}(\boldsymbol{Y} - \boldsymbol{T}) \Big] }{ \partial (\boldsymbol{Y} - \boldsymbol{T})^{\top}(\boldsymbol{Y} - \boldsymbol{T}) } }_{ I } \;
\underbrace{ \displaystyle \frac{ \partial (\boldsymbol{Y} - \boldsymbol{T})^{\top}(\boldsymbol{Y} - \boldsymbol{T}) }{ \partial (\boldsymbol{Y} - \boldsymbol{T}) } }_{ 2(\boldsymbol{Y} - \boldsymbol{T}) } \;
\underbrace{ \displaystyle \frac{ \partial (\boldsymbol{Y} - \boldsymbol{T}) }{ \partial \tilde{\boldsymbol{W}} } }_{ \tilde{\boldsymbol{X}}^{\top} } \\
&= \frac{1}{2} \cdot 2\tilde{\boldsymbol{X}}^{\top}(\boldsymbol{Y} - \boldsymbol{T}) = \frac{1}{2} \cdot 2\tilde{\boldsymbol{X}}^{\top}(\tilde{\boldsymbol{X}}\tilde{\boldsymbol{W}}- \boldsymbol{T})
\end{align}
$$
令其为零，就能得到一个十分眼熟的结果：$\tilde{\boldsymbol{W}}^{*} = (\tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}})^{-1}\tilde{\boldsymbol{X}}^{\top}\boldsymbol{T}$，所以是的损失最小化的预测就是
$$
\boldsymbol{y}^{*}(\boldsymbol{x}) = \Big[\tilde{\boldsymbol{W}}^{*} \Big]^{\top}\tilde{\boldsymbol{x}} = \Big[ (\tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}})^{-1}\tilde{\boldsymbol{X}}^{\top}\boldsymbol{T} \Big]^{\top} \tilde{\boldsymbol{x}}.
$$

关于此的另一个有趣性质是，假如任意目标向量 $\boldsymbol{t}_{n}$ 满足 $\boldsymbol{a}^{\top}\boldsymbol{t}_{n} + b = 0$，则预测结果也满足这个性质，也即 $\boldsymbol{a}^{\top}\boldsymbol{y}(\boldsymbol{x}) + b = 0$。我们继续使用之前的矩阵记号，不难得到最小二乘解下所有输入向量的预测结果为
$$
\boldsymbol{Y} = \tilde{\boldsymbol{X}}  (\tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}})^{-1}\tilde{\boldsymbol{X}}^{\top}\boldsymbol{T} 
$$
按照目标向量满足的性质，我们得到目标矩阵 $\boldsymbol{T}$ 必须满足 $\boldsymbol{T}\boldsymbol{a} + b\boldsymbol{1}_{N \times 1} = 0$，其中 $\boldsymbol{1}_{N \times 1}$ 是形状为 $N \times 1$ 的全 $1$ 矩阵。我们的目标是 $\boldsymbol{Y}\boldsymbol{a} + b\boldsymbol{1}_{N \times 1} = 0$。我们算一下 $\boldsymbol{Y}\boldsymbol{a}$：
$$
\begin{align}
\boldsymbol{Ya} &= \tilde{\boldsymbol{X}}  (\tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}})^{-1}\tilde{\boldsymbol{X}}^{\top}\boldsymbol{T} \\
&= -b \tilde{\boldsymbol{X}}  (\tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}})^{-1}\tilde{\boldsymbol{X}}^{\top} \boldsymbol{1}_{N \times 1}.
\end{align}
$$
此时我们回忆一下 $\tilde{\boldsymbol{X}}$ 的定义，它是加了偏置的增广输入矩阵，这意味着它的第一列恰好就是 $\boldsymbol{1}_{N \times 1}$，因此一定有 $\boldsymbol{1}_{N \times 1} \in \text{Col}(\tilde{\boldsymbol{X}})$。接下来我们看中间的矩阵，上一章中我们知道它是一个投影矩阵，将输入向量投影至 $\tilde{\boldsymbol{X}}$ 的列空间。而现在 $\boldsymbol{1}_{N \times 1}$ 本身就在列空间中，因此就有 $\tilde{\boldsymbol{X}}  (\tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}})^{-1}\tilde{\boldsymbol{X}}^{\top} \boldsymbol{1}_{N \times 1} = \boldsymbol{1}_{N \times 1}$，于是我们就证明了 $\boldsymbol{Y}\boldsymbol{a} + b\boldsymbol{1}_{N \times 1} = 0$。

在我们先前的设定中，目标向量 $\boldsymbol{t}$ 是一个 one-hot 向量，这意味着它们所有分量的总和总为 $1$，这是一个线性的约束，因此这样设定下解出来的模型预测 $\boldsymbol{y}$ 也满足加和等于 $1$ 的性质。但需要注意的是我们尚不能将其看做概率。因为 $\boldsymbol{y}$ 中元素并不一定在 $(0, 1)$ 中。

即使使用最小二乘解多分类问题看起来很美妙，但它有若干严重问题：

1. 平方损失目标与噪声服从 Gauss 分布假设下的极大似然估计的目标相同，当数据不在服从 Gauss 分布时模型性能会大打折扣
2. 平方损失受离群点的影响很大，后者会严重影响模型，使其 **鲁棒性** 降低（如下图所示）。这是因为平方损失给每个训练点相同权重。远离分布中心的离群点本应被分配更低的权重。

<center>
<img src="./ch5/attachments/Pasted%20image%2020250618155036.png" style="zoom:50%;" alt=""/>
</center>
<center>最小二乘法得到的分类模型受离群值的影响很大</center>
