# 线性回归中的决策
## 1 确定预测的值
在上一节中我们使用了 Gauss 分布建模给定输入 $\boldsymbol{x}$ 以及从数据集中学习得到的系数向量 $\boldsymbol{w}_{\text{ML}}$ 和方差 $\sigma^{2}_{\text{ML}}$。但实际任务中我们往往需要得到一个**确定的值**，而不是一个概率分布。这就预示着我们得到这个确定的值的过程需要分为两步：第一步叫做**推断**，得到的是一个关于输入 $\boldsymbol{x}$ 的分布 $p(t|\boldsymbol{x})$；第二步是**决策**，也就是从得到的这个分布里面选出一个确定的值 $f(\boldsymbol{x})$。

在之前我们使用 Gauss 分布来建模这个分布时，一个很自然的选择是 $f(\boldsymbol{x}) = y(\boldsymbol{x}, \boldsymbol{w}_{\text{ML}})$，也就是 Gauss 分布的均值。但在某些情况下这个选择并不好。我们为了形式化这个选择的过程，明白某些估计在什么情况下时有效的，我们就有了**决策论**。

假设我们选择了一个 $f(\boldsymbol{x})$，想知道它离标签 $t$ 有多远——我们想选一个最好的 $f(\boldsymbol{x})$ 让它离标签越近越好。可是我们并不知道标签到底是多少——或者说我们只知道标签的分布，这样我们就只能尝试最小化误差的期望
$$
\mathbb{E}[L] = \iint \underbrace{ L(t, f(\boldsymbol{x})) }_{ 误差函数 }\underbrace{ p(\boldsymbol{x}, t) }_{ \boldsymbol{x} 和 t 的联合分布 } \, \mathrm d{\boldsymbol{x}} \,\mathrm{d}t \tag{D-Err}
$$
此时我们需要找一个最优的 $f^{*}$，使得$\mathbb{E}[L]$ 是最小的，我们就要用到**变分**这个工具。

## 2 变分和泛函导数
在分析和优化理论中，我们常常需要考虑一个函数的最值（例如极小曲面问题和最速降线问题）。而当其性质足够好的时候，我们能通过令“导数”为零来求得这个值。让我们从我们熟悉的对象开始。对于一个可微的实值函数 $y$，我们知道其在某点的微分是局部近似原函数的**线性映射**：

$$y(x + \epsilon) = y(x) + \displaystyle \frac{ \mathrm{d}y }{ \mathrm{d}x } \epsilon + o(\epsilon)  = y(x) + {\color{blue} \left\langle \displaystyle\frac{ \mathrm{d}y }{ \mathrm{d}x }, \epsilon \right\rangle_{\mathbb{R}} }   + o(\epsilon) \tag{Diff-1}$$多元实值函数也不例外： $$y(\boldsymbol{x} + \boldsymbol{\epsilon}) = y(\boldsymbol{x}) + \sum\limits_{i=1}^{D} \displaystyle \frac{ \partial y }{ \partial x_{i} } \epsilon_{i} + o(\|\boldsymbol{\epsilon}\|_{2}^{2}) = y(\boldsymbol{x}) + {\color{blue} \left\langle \nabla_{\boldsymbol{x}}y, \boldsymbol{\epsilon} \right\rangle_{\mathbb{R}^{n}} }  + o(\|\boldsymbol{\epsilon}\|_{2}^{2})\tag{Diff-n}$$

按照这个思路，我们可以给出某个泛函（也就是吃进去一个函数（或向量），吐出来一个实数的映射），只需要做对应函数空间中的内积。这就得到**泛函导数**所满足的条件 

$$F[y + \epsilon \eta] = F[y] + {\color{blue} \left\langle \frac{\delta F}{\delta y}, \epsilon\eta \right\rangle_{\mathcal{H}} }  + o(\epsilon)\tag{Diff-H-1}$$其中 $\mathcal{H}$ 是一个 Hilbert 空间（完备的内积空间。可以将其理解为某些具有同样的比较好性质的函数所在的空间），且要求中间的内积项当 $\epsilon$ 趋于零时必须也趋于零。假如我们考虑某区间上平方可积的实函数所构成的集合 $L^{2}$ ，我们可以验证它是一个 Hilbert 空间（在此我们不加证明，直接用这个条件），其上的内积是 $\displaystyle \big< f, g \big> = \int {f(x)g(x)} \, \mathrm d{x}$，这样就得到了显式的变分满足的条件 $$F[y + \epsilon \eta] = F[y] + \epsilon \int \frac{\delta F}{\delta y(x)} \eta(x) \, \mathrm d{x}+ o(\epsilon)\tag{Diff-H-2}$$对比多元实函数的情形，一阶项从求和变成积分，对应变元取值从离散变成连续，符合我们的直觉和预期。现在考虑泛函 $F[y] = \displaystyle \int G(y, x) \, \mathrm d{x}$，考虑添加扰动之后的输入 

$$
\begin{aligned} F[y + \epsilon \eta] &= \displaystyle \int G(y+\epsilon \eta, x) \, \mathrm d{x} \\ &= \int \left[ G(y, x) + \epsilon\frac{\partial G}{\partial y}\eta(x) + o(\epsilon) \right] \, \mathrm d{x} & \text{Taylor展开}\\ &= F[y] + \epsilon \int {\color{red} \frac{\partial G}{\partial y} } \eta(x) \, \mathrm d{x} + o(\epsilon) & 假设余项积分之后还是无穷小 \end{aligned} \tag{E-1}
$$

此时我们要求其泛函导数（红色）为零，只需要求 $\displaystyle \frac{ \partial G }{ \partial y }  = 0$ 即可。

## 3 求解 Gauss 分布假设下的最优决策函数
我们假设误差函数是平方误差，也就是 $L(t, f(\boldsymbol{x})) = (t - f(\boldsymbol{x}))^{2}$，此时我们令 

$$\displaystyle G(f, x) = \int L(t, f(\boldsymbol{x}))p(\boldsymbol{x}, t) \, \mathrm d{t} = \int (t - f(\boldsymbol{x}))^{2}p(\boldsymbol{x}, t) \, \mathrm d{t} $$ 

用我们在插曲中得到的结果 (E-1) ，有

$$
\frac{\delta \mathbb{E}[L]}{\delta f(\boldsymbol{x})} = \displaystyle \frac{ \partial G }{ \partial f } = 2\int (t - f(\boldsymbol{x}))p(\boldsymbol{x}, t) \, \mathrm d{t} 
$$
令其为零，就得到
$$
\begin{align*}
& \int tp(\boldsymbol{x}, t) \, \mathrm d{t}  = \int f^{*}(\boldsymbol{x})p(\boldsymbol{x}, t) \, \mathrm d{t} \\
\iff & \int tp(\boldsymbol{x}, t) \, \mathrm d{t} = f^{*}(\boldsymbol{x})p(\boldsymbol{x}) & 边缘概率归一化\\
\iff & f^{*}(\boldsymbol{x}) = \frac{1}{p(\boldsymbol{x})} \int tp(\boldsymbol{x}, t) \, \mathrm d{t} = \int tp(t|\boldsymbol{x}) \, \mathrm d{t} = \mathbb{E}_{t}[t|\boldsymbol{x}]. & \text{Bayes公式}
\end{align*}
$$
这个结果说明最优的决策 $f^{*}$ 选择固定了 $\boldsymbol{x}$ 下的标签值的条件期望。在我们之前对条件概率 $p(t|\boldsymbol{x})$ 的假设中，其条件期望的的确确就是 Gauss 分布中的均值 $y(\boldsymbol{x}, \boldsymbol{w})$。在本例中，我们考虑从所有的函数中选出一个最佳的 $f^{*}$ 但在实际情况中，我们的所有可选项受制于模型中的参数（这定义了所谓**假设空间**）。然而我们可以构造出一族足够强大的函数，使得它们的线性组合可以以任意高的精度逼近任意的连续函数（**通用近似定理**）。

## 4 另一种方案
除了使用变分法，我们还可以从**偏差**和**方差**的角度得到同样的结果。我们把平方损失写成

$$
\begin{align*}
(f(\boldsymbol{x}) - t)^{2} &= (f(\boldsymbol{x}) - \mathbb{E}_{t}[t|\boldsymbol{x}] + \mathbb{E}_{t}[t|\boldsymbol{x}]- t)^{2}\\
&= (f(\boldsymbol{x}) - \mathbb{E}_{t}[t|\boldsymbol{x}])^{2} + 2(f(\boldsymbol{x}) - \mathbb{E}_{t}[t|\boldsymbol{x}])(\mathbb{E}_{t}[t|\boldsymbol{x}]- t) + (\mathbb{E}_{t}[t|\boldsymbol{x}]- t)^{2}
\end{align*}
$$

然后 (D-Err) 就可以写成

$$
\begin{align*}
\mathbb{E}[L] &= \iint \Big[(f(\boldsymbol{x}) - \mathbb{E}_{t}[t|\boldsymbol{x}])^{2} + 2(f(\boldsymbol{x}) - \mathbb{E}_{t}[t|\boldsymbol{x}])(\mathbb{E}_{t}[t|\boldsymbol{x}]- t) \\ &\quad\quad\quad   + (\mathbb{E}_{t}[t|\boldsymbol{x}]- t)^{2}\Big]p(\boldsymbol{x}, t)\, \mathrm d{\boldsymbol{x}} \,\mathrm{d}t\\
&= \int (f(\boldsymbol{x}) - \mathbb{E}_{t}[t|\boldsymbol{x}])^{2}  p(\boldsymbol{x}) \left[ \int p(t|\boldsymbol{x}) \, \mathrm d{t} \right] \,\mathrm{d}\boldsymbol{x} \\ &\quad\quad\quad + 2 \int (f(\boldsymbol{x}) - \mathbb{E}_{t}[t|\boldsymbol{x}]) \underbrace{ \left[ \int(\mathbb{E}_{t}[t|\boldsymbol{x}]- t)p(t|\boldsymbol{x}) \,\mathrm{d}t \right] }_{ 0 } p(\boldsymbol{x})\,\mathrm{d}x
\\ &\quad\quad\quad + \int p(\boldsymbol{x}) \left[ \int (f(\boldsymbol{x}) - \mathbb{E}_{t}[t|\boldsymbol{x}])^{2}  p(t|\boldsymbol{x}) \,\mathrm{d}t \right]\,\mathrm{d}\boldsymbol{x}\\
&= \int \underbrace{ (f(\boldsymbol{x}) - \mathbb{E}_{t}[t|\boldsymbol{x}])^{2} }_{ \text{偏差}^{2} }  p(\boldsymbol{x}) \,\mathrm{d}\boldsymbol{x} + \underbrace{ \int \underbrace{ \text{var}[t|\boldsymbol{x}] }_{ 方差 }p(\boldsymbol{x}) \, \mathrm d{\boldsymbol{x}} }_{ 与 f 无关 } 
\end{align*}
$$

于是我们只要最小化第一项，由于 $f$ 可以任意取，显然有 $f^{*} = \mathbb{E}_{t}[t|\boldsymbol{x}]$。

## 5 补充与其他
除了平方损失之外，我们还可以用别的损失，例如 **Minkovski 损失**
$$
\mathbb{E}[L_{q}] = \iint |f(\boldsymbol{x}) - t|^{q}p(\boldsymbol{x}, t)\,\mathrm{d}\boldsymbol{x}\,\mathrm{d}t
$$

还有一件事，假如数据并不是单峰分布（假如它的分布有三个峰），那我们用 Gauss 分布假设（以及上面推出来的结果）将会得到很坏的结果。一个解决方式是用混合 Gauss 模型，顾名思义就是取多个 Gauss 分布的凸组合。另外对于分类问题也有类似地决策论概念，我们将在讲分类的章节中见到它们。
